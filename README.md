# Paper_review

----------------------------------------------------

### BERT.pptx
  * Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_. [[paper]](https://www.aclweb.org/anthology/N19-1423.pdf)

-----------------------------------------------------

### GPT-1.pptx
  * Radford, A., Narasimhan, K., Salimans, T., Sutskeveret, L. Improving language understanding by generative pre-training." (2018). [[paper]](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
  
-----------------------------------------------------

### LDA.pptx
  * Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. _Journal of machine Learning research_. [[paper]](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
  
-----------------------------------------------------
